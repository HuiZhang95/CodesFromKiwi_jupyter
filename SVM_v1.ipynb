{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc8135-af87-4b36-b7e7-edd44db8967e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0518744-02a5-4d47-ac34-d5d7c5ee6133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# k-nearest distance\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "iris_X, iris_y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "np.random.seed(0)\n",
    "indices = np.random.permutation(len(iris_X))\n",
    "iris_X_train = iris_X[indices[:-10]]\n",
    "iris_y_train = iris_y[indices[:-10]]\n",
    "iris_X_test = iris_X[indices[-10:]]\n",
    "iris_y_test = iris_y[indices[-10:]]\n",
    "# Create and fit a nearest-neighbor classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(iris_X_train, iris_y_train)\n",
    "test= knn.predict(iris_X_test)\n",
    "print(test)\n",
    "print(iris_y_test)\n",
    "count = 0\n",
    "for tmpi in range(len(test)):\n",
    "    if test[tmpi] == iris_y_test[tmpi]:\n",
    "        count += 1\n",
    "rate = count/len(test)\n",
    "print([count, len(test), rate])\n",
    "import matplotlib.pyplot as plt\n",
    "plt.clf()\n",
    "f1 = plt.figure()\n",
    "plt.plot(iris_X,iris_X,'ko')\n",
    "plt.plot(iris_X_train,iris_y_train ,'ro')\n",
    "plt.plot(iris_X_test,iris_y_test,'bo')\n",
    "\n",
    "idx = 1\n",
    "print(iris_X[indices[idx]],iris_X[indices[idx]],iris_X_train[idx],iris_y_train[idx])\n",
    "print(np.shape(iris_X_train))\n",
    "print(np.shape(iris_y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974166ef-e218-4fa3-b2e6-189ed31000a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# linear regression\n",
    "from sklearn import datasets, linear_model, neighbors\n",
    "\n",
    "X_digits, y_digits = datasets.load_digits(return_X_y=True)\n",
    "X_digits = X_digits / X_digits.max()\n",
    "\n",
    "n_samples = len(X_digits)\n",
    "\n",
    "X_train = X_digits[: int(0.9 * n_samples)]\n",
    "y_train = y_digits[: int(0.9 * n_samples)]\n",
    "X_test = X_digits[int(0.9 * n_samples) :]\n",
    "y_test = y_digits[int(0.9 * n_samples) :]\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "logistic = linear_model.LogisticRegression(max_iter=1000)\n",
    "\n",
    "print(\"KNN score: %f\" % knn.fit(X_train, y_train).score(X_test, y_test))\n",
    "print(\n",
    "    \"LogisticRegression score: %f\"\n",
    "    % logistic.fit(X_train, y_train).score(X_test, y_test)\n",
    ")\n",
    "\n",
    "print([np.shape(X_train),np.shape(y_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9e5c9c-098a-4f3f-8fd6-d4827c0e6603",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# logistic vs. k-nearest\n",
    "from sklearn import datasets, linear_model, neighbors\n",
    "\n",
    "X_digits, y_digits = datasets.load_digits(return_X_y=True)\n",
    "X_digits = X_digits / X_digits.max()\n",
    "\n",
    "n_samples = len(X_digits)\n",
    "\n",
    "X_train = X_digits[: int(0.9 * n_samples)]\n",
    "y_train = y_digits[: int(0.9 * n_samples)]\n",
    "X_test = X_digits[int(0.9 * n_samples) :]\n",
    "y_test = y_digits[int(0.9 * n_samples) :]\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "logistic = linear_model.LogisticRegression(max_iter=1000)\n",
    "\n",
    "print(\"KNN score: %f\" % knn.fit(X_train, y_train).score(X_test, y_test))\n",
    "print(\n",
    "    \"LogisticRegression score: %f\"\n",
    "    % logistic.fit(X_train, y_train).score(X_test, y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7cb72c-1db0-4192-9e65-442415381aba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51efeec8-0d6e-4a2a-a1c3-ad30981d3ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# support vector machine (SVM)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "# Take the first two features. We could avoid this by using a two-dim dataset\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "C = 1.0  # SVM regularization parameter\n",
    "models = (\n",
    "    svm.SVC(kernel=\"linear\", C=C),\n",
    "    svm.LinearSVC(C=C, max_iter=10000, dual=\"auto\"),\n",
    "    svm.SVC(kernel=\"rbf\", gamma=0.7, C=C),\n",
    "    svm.SVC(kernel=\"poly\", degree=3, gamma=\"auto\", C=C),\n",
    ")\n",
    "models = (clf.fit(X, y) for clf in models)\n",
    "\n",
    "# title for the plots\n",
    "titles = (\n",
    "    \"SVC with linear kernel\",\n",
    "    \"LinearSVC (linear kernel)\",\n",
    "    \"SVC with RBF kernel\",\n",
    "    \"SVC with polynomial (degree 3) kernel\",\n",
    ")\n",
    "\n",
    "# Set-up 2x2 grid for plotting.\n",
    "fig, sub = plt.subplots(2, 2)\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "\n",
    "for clf, title, ax in zip(models, titles, sub.flatten()):\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(\n",
    "        clf,\n",
    "        X,\n",
    "        response_method=\"predict\",\n",
    "        cmap=plt.cm.coolwarm,\n",
    "        alpha=0.8,\n",
    "        ax=ax,\n",
    "        xlabel=iris.feature_names[0],\n",
    "        ylabel=iris.feature_names[1],\n",
    "    )\n",
    "    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors=\"k\")\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd917bd-2ff3-4f56-a566-b9b74dbc52b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# svm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets, svm\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X = X[y != 0, :2]\n",
    "y = y[y != 0]\n",
    "\n",
    "n_sample = len(X)\n",
    "\n",
    "np.random.seed(0)\n",
    "order = np.random.permutation(n_sample)\n",
    "X = X[order]\n",
    "y = y[order].astype(float)\n",
    "\n",
    "X_train = X[: int(0.9 * n_sample)]\n",
    "y_train = y[: int(0.9 * n_sample)]\n",
    "X_test = X[int(0.9 * n_sample) :]\n",
    "y_test = y[int(0.9 * n_sample) :]\n",
    "\n",
    "# fit the model\n",
    "for kernel in (\"linear\", \"rbf\", \"poly\"):\n",
    "    clf = svm.SVC(kernel=kernel, gamma=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.clf()\n",
    "    plt.scatter(\n",
    "        X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired, edgecolor=\"k\", s=20\n",
    "    )\n",
    "\n",
    "    # Circle out the test data\n",
    "    plt.scatter(\n",
    "        X_test[:, 0], X_test[:, 1], s=80, facecolors=\"none\", zorder=10, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    plt.axis(\"tight\")\n",
    "    x_min = X[:, 0].min()\n",
    "    x_max = X[:, 0].max()\n",
    "    y_min = X[:, 1].min()\n",
    "    y_max = X[:, 1].max()\n",
    "\n",
    "    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(XX.shape)\n",
    "    plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "    plt.contour(\n",
    "        XX,\n",
    "        YY,\n",
    "        Z,\n",
    "        colors=[\"k\", \"k\", \"k\"],\n",
    "        linestyles=[\"--\", \"-\", \"--\"],\n",
    "        levels=[-0.5, 0, 0.5],\n",
    "    )\n",
    "\n",
    "    plt.title(kernel)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e6c783-52ab-44fd-b94e-91d3c3f3110f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, svm\n",
    "X_digits, y_digits = datasets.load_digits(return_X_y=True)\n",
    "svc = svm.SVC(C=1, kernel='linear')\n",
    "svc.fit(X_digits[:-100], y_digits[:-100]).score(X_digits[-100:], y_digits[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2454cb41-3bf0-4195-867d-60be98454a7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split data into folds\n",
    "import numpy as np\n",
    "X_folds = np.array_split(X_digits, 4)\n",
    "y_folds = np.array_split(y_digits, 4)\n",
    "scores = list()\n",
    "for k in range(4):\n",
    "    # We use 'list' to copy, in order to 'pop' later on\n",
    "    X_train = list(X_folds)\n",
    "    X_test = X_train.pop(k)\n",
    "    X_train = np.concatenate(X_train)\n",
    "    y_train = list(y_folds)\n",
    "    y_test = y_train.pop(k)\n",
    "    y_train = np.concatenate(y_train)\n",
    "    scores.append(svc.fit(X_train, y_train).score(X_test, y_test))\n",
    "print(scores)\n",
    "print(type(X_digits))\n",
    "print(np.shape(X_digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53167406-2190-4ef0-8e1f-798d6db4e316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cross validation\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "X = [\"a\", \"a\", \"a\", \"b\", \"b\", \"c\", \"c\", \"c\", \"c\", \"c\"]\n",
    "k_fold = KFold(n_splits=5)\n",
    "for train_indices, test_indices in k_fold.split(X):\n",
    "    print('Train: %s | test: %s' % (train_indices, test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82739112-e22c-4f2e-bd85-f752ece0cf14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify N folds cross validation. By default it is 5 folds\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "k_fold = KFold(n_splits=6)\n",
    "[svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test]) for train, test in k_fold.split(X_digits)]\n",
    "\n",
    "cross_val_score(svc, X_digits, y_digits, cv=k_fold, n_jobs=-1)\n",
    "# or\n",
    "cross_val_score(svc, X_digits, y_digits, cv=k_fold, scoring='precision_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d0aa0c-3341-45c8-9c5d-eac5f7deda49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "X, y = datasets.load_digits(return_X_y=True)\n",
    "svc = svm.SVC(kernel=\"linear\")\n",
    "C_s = np.logspace(-10, 0, 10)\n",
    "print(C_s)\n",
    "scores = list()\n",
    "scores_std = list()\n",
    "for C in C_s:\n",
    "    svc.C = C\n",
    "    this_scores = cross_val_score(svc, X, y, n_jobs=1)\n",
    "    scores.append(np.mean(this_scores))\n",
    "    scores_std.append(np.std(this_scores))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogx(C_s, scores)\n",
    "plt.semilogx(C_s, np.array(scores) + np.array(scores_std), \"b--\")\n",
    "plt.semilogx(C_s, np.array(scores) - np.array(scores_std), \"b--\")\n",
    "locs, labels = plt.yticks()\n",
    "plt.yticks(locs, list(map(lambda x: \"%g\" % x, locs)))\n",
    "plt.ylabel(\"CV score\")\n",
    "plt.xlabel(\"Parameter C\")\n",
    "plt.ylim(0, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af26207a-71ca-4dea-80c6-921880163321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grid search\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "Cs = np.logspace(-6, -1, 10)\n",
    "clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs), n_jobs=-1)\n",
    "clf.fit(X_digits[:1000], y_digits[:1000])      \n",
    "print(clf.best_score_) \n",
    "print(clf.best_estimator_.C)                            \n",
    "# Prediction performance on test set is not as good as on train set\n",
    "clf.score(X_digits[1000:], y_digits[1000:]) \n",
    "\n",
    "# Nested cross-validation\n",
    "cross_val_score(clf, X_digits, y_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b039633-0894-4818-98e9-8838730d7cf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc898cd-582d-4998-8477-7722b224c08f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a full script of performing nested cross validation, including modifying folds, grid search\n",
    "\n",
    "# specify svc with n folds\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "fold_no = 10\n",
    "k_fold = KFold(n_splits=fold_no)\n",
    "svc = svm.SVC(kernel=\"linear\")\n",
    "[svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test]) for train, test in k_fold.split(X_digits)]\n",
    "\n",
    "# grid search \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# creating grid\n",
    "#param_grid = [{'class_weight': [None, 'balanced'], \n",
    "#               'C':[0.5, 1, 10, 100], \n",
    "#               'gamma': ['scale', 1, 0.1, 0.01, 0.001],\n",
    "#               'kernel': ['rbf', 'poly', 'sigmoid','linear'], \n",
    "#               'decision_function_shape': ['ovo','ovr']}]\n",
    "\n",
    "Cs = np.logspace(-6, -1, 30)\n",
    "clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs), n_jobs=-1)\n",
    "clf.fit(X_digits[:1000], y_digits[:1000])     \n",
    "\n",
    "# perform nested cross validation\n",
    "cross_val_score(clf, X_digits, y_digits, cv=k_fold) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4fdf42-badd-4a56-8364-cbc9b78df83e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### can also be as simple as the following code\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "# grid search\n",
    "#svc = svm.SVC(kernel=\"linear\") # one of course can specify other parameters\n",
    "Cs = np.logspace(-6, -1, 30)\n",
    "clf = GridSearchCV(estimator=svm.SVC(), param_grid=dict(C=Cs), n_jobs=-1)\n",
    "clf.fit(X_digits[:1000], y_digits[:1000])     \n",
    "\n",
    "# perform nested cross validation\n",
    "cross_val_score(clf, X_digits, y_digits, cv=k_fold) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633ba5f-ec89-4656-9972-a4af4c52677a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# approach 1 and two genarate the same result. It means that after grid search, it will automatically select the best parameters\n",
    "# approach 1\n",
    "clf.predict(X_digits[1000:])\n",
    "# approach 2\n",
    "svc = svm.SVC(C=clf.best_params_.get('C'), kernel='linear')\n",
    "svc.fit(X_digits[:1000], y_digits[:1000])\n",
    "svc.predict(X_digits[1000:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d6b4e-ba99-4d92-82ce-6a067293f113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(clf.best_params_.get('C'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24825b8-78b5-437d-b56f-359f0dc07634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "svc = svm.SVC(C=clf.best_params_.get('C'), kernel='linear')\n",
    "[svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test]) for train, test in k_fold.split(X_digits)]\n",
    "cross_val_score(svc, X_digits, y_digits, cv=k_fold) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acbba27-eb2b-40ed-8bc0-a9886e7a9a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation estimator\n",
    "\n",
    "from sklearn import linear_model, datasets\n",
    "lasso = linear_model.LassoCV()\n",
    "X_diabetes, y_diabetes = datasets.load_diabetes(return_X_y=True)\n",
    "lasso.fit(X_diabetes, y_diabetes)\n",
    "# The estimator chose automatically its lambda:\n",
    "lasso.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39846c20-7278-44df-8d27-96edff09d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of cross validation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X, y = datasets.load_diabetes(return_X_y=True)\n",
    "X = X[:150]\n",
    "y = y[:150]\n",
    "\n",
    "lasso = Lasso(random_state=0, max_iter=10000)\n",
    "alphas = np.logspace(-4, -0.5, 30)\n",
    "\n",
    "tuned_parameters = [{\"alpha\": alphas}]\n",
    "n_folds = 5\n",
    "\n",
    "clf = GridSearchCV(lasso, tuned_parameters, cv=n_folds, refit=False)\n",
    "clf.fit(X, y)\n",
    "scores = clf.cv_results_[\"mean_test_score\"]\n",
    "scores_std = clf.cv_results_[\"std_test_score\"]\n",
    "\n",
    "# plot scores\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "plt.semilogx(alphas, scores)\n",
    "\n",
    "std_error = scores_std / np.sqrt(n_folds)\n",
    "\n",
    "plt.semilogx(alphas, scores + std_error, \"b--\")\n",
    "plt.semilogx(alphas, scores - std_error, \"b--\")\n",
    "\n",
    "# alpha=0.2 controls the translucency of the fill color\n",
    "plt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2)\n",
    "\n",
    "plt.ylabel(\"CV score +/- std error\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.axhline(np.max(scores), linestyle=\"--\", color=\".5\")\n",
    "plt.xlim([alphas[0], alphas[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fde0e3-105f-41bd-be77-a3d6a0a238e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "lasso_cv = LassoCV(alphas=alphas, random_state=0, max_iter=10000)\n",
    "k_fold = KFold(3)\n",
    "\n",
    "print(\"Answer to the bonus question:\", \"how much can you trust the selection of alpha?\")\n",
    "print()\n",
    "print(\"Alpha parameters maximising the generalization score on different\")\n",
    "print(\"subsets of the data:\")\n",
    "for k, (train, test) in enumerate(k_fold.split(X, y)):\n",
    "    lasso_cv.fit(X[train], y[train])\n",
    "    print(\n",
    "        \"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".format(\n",
    "            k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test])\n",
    "        )\n",
    "    )\n",
    "print()\n",
    "print(\"Answer: Not very much since we obtained different alphas for different\")\n",
    "print(\"subsets of the data and moreover, the scores for these alphas differ\")\n",
    "print(\"quite substantially.\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0dd847-1206-4baf-a7f8-dcf698ee6d98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### can also be as simple as the following code\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# load in data\n",
    "X_digits, y_digits = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "\n",
    "gridSearch_no = 5\n",
    "n_samples = len(X_digits)\n",
    "BestCore = np.empty(gridSearch_no)\n",
    "for i in range(gridSearch_no):\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    X_gridSearch = X_digits[indices[: int(0.9 * n_samples)]]\n",
    "    y_gridSearch = y_digits[indices[: int(0.9 * n_samples)]]\n",
    "    \n",
    "    # grid search\n",
    "    #svc = svm.SVC(kernel=\"linear\") # one of course can specify other parameters\n",
    "    Cs = np.logspace(-6, -1, 30)\n",
    "    clf = GridSearchCV(estimator=svm.SVC(), param_grid=dict(C=Cs), n_jobs=-1)\n",
    "    clf1 = make_pipeline(StandardScaler(), clf)\n",
    "    clf1.fit(X_gridSearch, y_gridSearch)     \n",
    "    #print(dir(clf1))\n",
    "\n",
    "    # perform nested cross validation\n",
    "    k_fold_no = 6\n",
    "    k_fold = KFold(k_fold_no, shuffle=True)\n",
    "    clf2 = make_pipeline(StandardScaler(), clf1)\n",
    "    cross_val_score(clf2, X_digits, y_digits, cv=k_fold) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15fcaefc-1ad8-4c04-ac6c-c402b04c90b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### can also be as simple as the following code\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# load in data\n",
    "X_digits, y_digits = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "gridSearch_no = 5\n",
    "n_samples = len(X_digits)\n",
    "selectRatio = 0.9 #randomly select 90% of data for training\n",
    "BestCore = np.empty(gridSearch_no)\n",
    "for i in range(gridSearch_no):\n",
    "    # select data for grid search\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    X_train = X_digits[indices[: int(selectRatio * n_samples)]]\n",
    "    y_train = y_digits[indices[: int(selectRatio * n_samples)]]\n",
    "    X_test = X_digits[indices[int(selectRatio * n_samples) :]]\n",
    "    y_test = y_digits[indices[int(selectRatio * n_samples) :]]\n",
    "\n",
    "    # standadize the data\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    # grid search\n",
    "    #svc = svm.SVC(kernel=\"linear\") # one of course can specify other parameters\n",
    "    Cs = np.logspace(-6, -1, 30)\n",
    "    clf = GridSearchCV(estimator=svm.SVC(), param_grid=dict(C=Cs), n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)    \n",
    "    #print(dir(clf1))\n",
    "\n",
    "    # test the model\n",
    "    #prid_result = clf.predict(X_test)\n",
    "    tmpscore = clf.score(X_test,y_test)\n",
    "    \n",
    "    BestCore[i] = tmpscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d3b6913-75c7-4865-b34a-1203bb1d61ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab149f99-4a88-410c-8f38-55dca6a70505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
